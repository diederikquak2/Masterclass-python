{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = pd.read_json('data/activities.json', convert_dates=['start_date_local'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Simple and Useful DataFrame Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    activity_df\n",
    "    .sort_values('start_date_local', ascending=False)\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(activity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: in evaluation of the above expression, is any `DataFrame` method called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of Concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_activities():\n",
    "    activity_df = pd.read_json('data/activities.json', convert_dates=['start_date_local'])\n",
    "    runs_sorted_by_date_df = (\n",
    "        activity_df\n",
    "        .loc[lambda df: df['type'] == 'Run']\n",
    "        .sort_values('start_date_local')\n",
    "    )\n",
    "    runs_sorted_by_date_df.to_csv('data/processed_activities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_activities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "\n",
    "- What if there are many of these processes, and the data source changes from .json file to database?\n",
    "- How can we test the business logic without access to file system or database?\n",
    "- How can this be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm data/processed_activities.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivitySource(ABC):\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class JsonLocalActivitySource(ActivitySource):\n",
    "    filename: str = 'data/activities.json'  # will usually come from a config file\n",
    "    \n",
    "    def load(self):\n",
    "        return pd.read_json(self.filename, convert_dates=['start_date_local'])\n",
    "\n",
    "@dataclass\n",
    "class PostgressActivitySource(ActivitySource):\n",
    "    db_connection: Any  # will usually come from a config file\n",
    "    \n",
    "    def load(self):\n",
    "        # pseudo-code\n",
    "        return db_connection.query('SELECT * FROM activities')\n",
    "\n",
    "class ActivitySink(ABC):\n",
    "    @abstractmethod\n",
    "    def save(self, activity_df: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class CsvLocalActivitySink(ActivitySink):\n",
    "    filename: str = 'data/processed_activities.csv'  # will usually come from a config file\n",
    "    \n",
    "    def save(self, activity_df: pd.DataFrame):\n",
    "        activity_df.to_csv(self.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_runs_and_sort_by_date(activity_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (\n",
    "        activity_df\n",
    "        .loc[lambda df: df['type'] == 'Run']\n",
    "        .sort_values('start_date_local')\n",
    "    )\n",
    "\n",
    "def process_activities(\n",
    "    source: ActivitySource,\n",
    "    processing_fn: Callable[[pd.DataFrame], pd.DataFrame],\n",
    "    sink: ActivitySink\n",
    ") -> None:\n",
    "    sink.save(processing_fn(source.load()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_activities(JsonLocalActivitySource(), select_runs_and_sort_by_date, CsvLocalActivitySink())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: architecture picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: for large/complex workflows, consider tools such as [Apache Airflow](https://airflow.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decluttering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df['year'] = activity_df['start_date_local'].dt.year\n",
    "activity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_2018_df = activity_df.loc[activity_df['year'] == 2018]\n",
    "activities_2018_by_type = activities_2018_df.groupby('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_velocity_by_type = activities_2018_by_type['velocity_mean'].mean()\n",
    "mean_velocity_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = pd.read_json('data/activities.json', convert_dates=['start_date_local'])\n",
    "(\n",
    "    activity_df\n",
    "    .loc[lambda df: df['start_date_local'].dt.year == 2018]\n",
    "    .groupby('type')['velocity_mean'].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing ... `Groupby.transform()`, TODO link to docs of agg/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    activity_df\n",
    "    .loc[lambda df: df['start_date_local'].dt.year == 2018]\n",
    "    .groupby('type')['velocity_mean'].transform('mean')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    activity_df\n",
    "    .loc[lambda df: df['start_date_local'].dt.year == 2018]\n",
    "    .assign(v_mean_diff=lambda df: (\n",
    "        df.groupby('type')['velocity_mean']\n",
    "        .transform(lambda type_v: type_v - type_v.mean())))\n",
    "    .sample(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readable, Testable Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_non_warmup(activity_df):\n",
    "    return (\n",
    "        (activity_df['elapsed_time'] > 600) |\n",
    "        (activity_df['heartrate_mean'] > 150)\n",
    "    )\n",
    "\n",
    "def select_non_warmup_runs(activity_df):\n",
    "    return (\n",
    "        activity_df\n",
    "        .loc[is_non_warmup]\n",
    "        .loc[lambda df: df['type'] == 'Run']\n",
    "    )\n",
    "\n",
    "def to_z_score(series):\n",
    "    # What if series has a lenght of 1?\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "def add_z_score(activity_df, column):\n",
    "    return activity_df.assign(**{f'{column}_z': lambda df: to_z_score(df[column])})\n",
    "\n",
    "def best_n_years(activity_df, metrics=['velocity_mean'], n_years=1):\n",
    "    return (\n",
    "        activity_df\n",
    "        .assign(year=lambda df: df['start_date_local'].dt.year)\n",
    "        .groupby('year')[metrics].mean()\n",
    "        .sort_values(metrics, ascending=False)\n",
    "        .head(n_years)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    activity_df\n",
    "    .pipe(select_non_warmup_runs)\n",
    "    .pipe(add_z_score, column='velocity_mean')\n",
    "    .pipe(best_n_years, metrics=['velocity_mean_z', 'velocity_mean'], n_years=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Is this a reliable analysis? What can be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Don't Repeat Yourself](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)  # You may need to restart the notebook to make this work\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info(df):\n",
    "    if df is not None:\n",
    "        column_info = ','.join([f'{col} ({df[col].isnull().sum()} missing)' for col in df])\n",
    "        return f'rows: {len(df)}, columns: {column_info}'\n",
    "    else:\n",
    "        return '<None>'\n",
    "\n",
    "def pandas_pipe_logging(func):\n",
    "    @wraps(func)\n",
    "    def logging_wrapper(*args, **kwargs):\n",
    "        in_df = args[0]\n",
    "        logger.debug(f'Calling pipeline function {func.__name__} with input {df_info(in_df)}')\n",
    "        start = time.time()\n",
    "        out = func(*args, **kwargs)\n",
    "        stop = time.time()\n",
    "        logger.debug(f'Returning dataframe with {df_info(out)}')\n",
    "        logger.debug(f'Took {stop - start:.4f}s')\n",
    "        return out\n",
    "\n",
    "    return logging_wrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
